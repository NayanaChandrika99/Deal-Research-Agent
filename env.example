# LLM API Configuration
# Copy this file to .env and fill in your actual API keys

# Cerebras API (https://inference-docs.cerebras.ai/)
# Fast inference with Llama 3.1 8B (~2200 tokens/sec)
CEREBRAS_API_KEY=your-cerebras-api-key-here
CEREBRAS_BASE_URL=https://api.cerebras.ai/v1

# OpenAI API (for embeddings - Cerebras doesn't provide embeddings yet)
OPENAI_API_KEY=sk-your-openai-api-key-here

# DSPy Configuration
# Using Cerebras Llama 3.1 8B for reasoning and optimization
DSPY_MODEL=llama3.1-8b
DSPY_API_BASE=https://api.cerebras.ai/v1
DSPY_API_KEY=${CEREBRAS_API_KEY}
DSPY_TEMPERATURE=0.7
DSPY_MAX_TOKENS=2000

# Embedding Configuration (still using OpenAI for embeddings)
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSION=1536
EMBEDDING_API_KEY=${OPENAI_API_KEY}

# LLM-as-Judge Configuration (for prompt optimization metrics)
# Using same Cerebras model for cost efficiency
JUDGE_MODEL=llama3.1-8b
JUDGE_API_BASE=https://api.cerebras.ai/v1
JUDGE_API_KEY=${CEREBRAS_API_KEY}
JUDGE_TEMPERATURE=0.3

# Rate Limiting (Cerebras Free Tier: 30 req/min, 60k input tokens/min)
# Adjust based on your tier (Developer: 1K req/min, 1M input tokens/min)
MAX_REQUESTS_PER_MINUTE=30
MAX_INPUT_TOKENS_PER_MINUTE=60000

# Optional: Logging
LOG_LEVEL=INFO

# Context Window Settings
# Free tier: 8k context, 8k max output
# Paid tiers: 32k context, 8k max output
MAX_CONTEXT_LENGTH=8192
MAX_OUTPUT_TOKENS=8192

